---
title: "GSCApaper_markdown"
author: "DVM Bishop"
date: "19/05/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(doBy)
library(tidyr)
library(tidyverse)
library(MASS)
library(stats)
library(doSNOW)
library(foreach)
library(ASGSCA)
library(matrixcalc)
library(tictoc)
library(ggcorrplot)
library(ggplot2)

options(scipen = 999) #turn off scientific notation
```

## R Markdown

Combines various scripts used for the paper. 
Based on *simulating gsca data via latent variable_v2.R* from OSF.

Data simulated either by (a) generating data with given correlation structure; or (b) Romdhani method.
Varied:
N SNPs; 
N SNPs with an effect on phenotype; 
N participants; 
N genes; (i.e. N gene pathways in GSCA)
effect size, i.e. size of correlation between SNPs with an effect and phenotypes. 
Here used small effect size (r=.1) and large (r=.4)
Also contrasted situations where the SNPs with an effect were clustered within a gene, or spread across genes.

Because GSCA is very slow, did 100 runs for each combination (this takes several weeks to run through all combinations)



```{r initialisevars}

nrun<-100
startrow<-1 #row of big summary to write to
mynperm=100 #for GSCA - probably need more than this but to test prog use 100 for speed

npop<-10000 #size of population to simulate
n2sim<-3 #n phenos indexing latent pheno factor : nb SNPS correlate with latent factor

LDbase <- .75 #This is the highest correlation between adjacent SNPs - correl will decline with distance
#original file had max LD of .9. Later I tried with reduced value of .75

phencorr<-.75 #correlation of 3 phenotypes with latent variable
#with this setting, intercorrelation between phenotypes is around .56

effsizelist<-c(1,4) #will be divided by 10 to give a correlation
#(just given here as whole number to make easier to use in filename)

nsnpefflist<-c(0,5,10) #N of SNPs with an effect on phenotype (NB not proportion: proportion decreases as N SNP increases)
ngenelist<-c(1) 
nsublist<-c(100,500)
nsnp_per_gene<-c(20,40,100) #N SNPs that are tested - only some will have effect

bigsumname<-'bigsum_nsnp20|40|100_eff0|10|40_peff20|40_ngen1_nsub100|500_run100' #name for this big summary file
#better to automate name creation by string concatenation!

effsizelist<-effsizelist/10 #effect size as correlation
```

```{r createdataframeforsummary}
bigsumNrow<-nrun*length(effsizelist)*length(nsnpefflist)*length(nsublist)*sum(ngenelist)*length(nsnp_per_gene)
#NB we have to do 'sum' for ngenelist, as there is a row for each gene
bigsummary<-data.frame(matrix(NA,ncol=12,nrow=bigsumNrow))
colnames(bigsummary)<-c('run','cluster','nsnp','ngenes','nsnpeff','effsize','nsub','condition','gene','path','p','sig.p')

```

We start by simulating a large population with the desired correlational structure. 

We have 3 language phenotypes, which are indicators of a latent factor. The correlation of each of these with the latent factor is specified above as phencorr. 

We create a correlation matrix for the desired N SNPs. Initially this is just a matrix of zeros with ones on diagonal, but to have more realistic data, we can simulate a degree of linkage disequilibrium, i.e. adjacent SNPs are correlated.
The variable LDbase is used to control this - a value of .75 gives realistic-looking values. 

Default assumption is no correlation of SNPs with latent factor; then specified number of SNPs have effect added by changing their correlation with latent value to equal the specific effect size.

Initially, the SNP values are just simulated together with the latent factor using mvrnorm to give a matrix of z-scores. This matrix will have the SNPs correlated according to the setting of LDbase, and individual SNPs will also be correlated with the latent factor, with level of effect size, as specified above.

We then need to adjust the SNPs scores so that they correspond to values of 0, 1 or 2 - i.e. N major alleles. We select a random value between .25 and .5 for minor allele frequency for each SNP, and from this we can compute the proportion of each genotype in the population.  We allocate a score of 0, 1 or 2, depending on the z-score for the simulated data (using normal distribution of probabilities). E.g. if we expected 50% of cases to have AA genotype (equivalent to SNP score of 2) we would allocate a score of 2 when the z-score was greater than zero (p = .5).

Finally, we need to simulate z-scores for observed phenotype variables. We simply simulate these as scores correlated with the latent variable by the specified amount (phencorr) - this is done using a formula that treats each score as the sum of the latent variable plus an error term. 

The end result of this step is a dataframe consisting of a row for each simulated person in the population (currently set at 10,000 cases), which contains values of 0, 1 or 2 for each SNP, value for a latent phenotype variable, and values for 3 observed phenotypes. This should have the desired correlation structure - which can be observed in a heatmap using  ggcorrplot(cor(mydata)).

The simulated data is saved so it can be used to compare different analyses.

          

```{r makedatafunction}   
makedata <- function(nsnp,ngenes,effsize,nsnpeff,LDbase){


 
     gpcov<-effsize #effect size of SNPs with effect, set as correlation 
    snpnames<-paste0('snp', 1:nsnp)
    maf<-runif(nsnp,.25,.5) #minor allele freq set to random value from .25 to .5
    
    #Setup a dataframe to hold to hold simulated data.
    mydata<-data.frame(matrix(nrow=npop,ncol=(4+sum(nsnp)))) #set to hold npop cases - we will sample from these for each run
    #4 additional columns with hold (a)latent phenotype and (b) 3 component variables
    #SNP values are 0, 1 or 2, but we start with random normal numbers
    
    #-----------------------------------------------------------------
    #simulate the SNPS as correlated zscores, correlation is mycorr
    #-----------------------------------------------------------------
    #Where there is correlation, assume correlation depends on how close SNPs are, 
    #achieved by making size of correlation proportional to distance
    # (where sequence order is proxy for distance)
    
    
    h <-nsnp+1 #an additional column used for latent variable
    mymean<-rep(0,h) #vector of means to use in mvrnorm function
    mycorr<-0 #default is uncorrelated
    mycov2<-matrix(mycorr,nrow=h,ncol=h) #cov matrix for mvrnorm
    
    diag(mycov2)<-rep(1,h) #ones on diagonal of cov matrix
    gene_break<-floor(nsnp/ngenes)

    #Now specify correlation between SNPs, i.e. linkage disequilibrium       
    if(LDbase>0){ #need to compute correlation for conditions where SNP effects clustered in genes
      for (i in 1:h){
        irange<-1+as.integer((i-1)/gene_break) #irange specifies haplotype block for i
        for (j in 1:h){
          jrange<- 1+as.integer((j-1)/gene_break) #jrange specifies haplotype block for j
          if(irange==jrange){
            k<- abs(i-j)
            thisr<-ifelse(nsnp<=20,LDbase-gene_break*k/100,(LDbase-gene_break*k/((nsnp^2)/(10*ngenes)))) #tweaked so magnitude of correlation declines with distance between SNPs (PT: further tweak to allow different nsnp without problems.)
            if(thisr<0){thisr<-0}
            mycov2[i,j]<-thisr
            mycov2[j,i]<-mycov2[i,j] #correlation determined by distance between SNPs!
            if(k==0){mycov2[i,j]<-1}
          }
        }
      }
    }
    
    #Identify which SNPs are correlated with latent phenotype
    #Here assume effects divided equally betewen genes
    
    nsnp_g<-round(nsnp/ngenes,0)
    nsnpeff_g<-round(nsnpeff/ngenes,0)
    snpeff_pos<-vector() #initialise vector for holding indices of SNPs with effect
    for (g in 1:ngenes){
      myrange<-(1+(g-1)*nsnp_g):(g*nsnp_g) #range of indices for SNPs in this gene
      snpeff_pos<-c(snpeff_pos,sample(myrange,nsnpeff_g,replace=F)) #randomly selected SNPs in this range
    }
    mycov2[snpeff_pos,h]<-effsize #for SNPs with effect, add effect size in cov matrix
    mycov2[h,snpeff_pos]<-effsize
    if(matrixcalc::is.positive.definite(mycov2)==FALSE)
    {mycov2<-Matrix::nearPD(mycov2,keepDiag=TRUE)$mat}
    
    #create simulated data for SNPs +latent phenotype only
    mymean<-rep(0,h)
    mydata=data.frame(mvrnorm(n = npop, mymean, mycov2))
    colnames(mydata)[h]<-c("latentpheno")
    #################################################
    
    colnames(mydata)[1:nsnp]<-snpnames
    
    #values for phenotype created as correlated with latent: r = phencorr
    #formula creates correlated variables as sum of latent and random error
    mydata$nonword<-phencorr*mydata$latent+sqrt(1-phencorr^2)*rnorm(nrow(mydata))
    mydata$elang<-phencorr*mydata$latent+sqrt(1-phencorr^2)*rnorm(nrow(mydata))
    mydata$rlang<-phencorr*mydata$latent+sqrt(1-phencorr^2)*rnorm(nrow(mydata))
    
    
    #-------------------------------------------------------------------------
    #Convert gene scores to integers: 0-2 for N SNPs with major allele
    
    firstcol<-1
    lastcol<-nsnp
    p<-c(0,0,0) #initialise a vector to hold p values for different N alleles
    for (i in 1:nsnp){
      p[1]<-(1-maf[i])^2 #proportion homozygous for major allele
      p[2]<-2*(1-maf[i])*maf[i] #proportion heterozygous
      p[3]<-maf[i]^2 #proportion homozygous for minor allele
      
      #now use p-values to assign z-score cutoffs that convert to 0,1,2 minor alleles
      temp<-mydata[,i] #column of z-scores for this SNP
      w<-which(temp<qnorm(p[1]))
      mydata[w,i]<-0
      w<-which(temp>qnorm(p[1]))
      mydata[w,i]<-1
      w<-which(temp>qnorm(p[2]+p[1]))
      mydata[w,i]<-2
    }
    return(mydata) 
  }
```           
 


```{r adddata}
#ensure makedata function is loaded before running this chunk
 mycondition<-0 #initialise
 tic() #start timer
  for (nsnpg in nsnp_per_gene){
     for (effsize in effsizelist){
       for(nsnpeff in nsnpefflist){ #cycle by N SNPs with an effect
         for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
       
 #   nsnp=20;effsize=.1;nsnpeff=5;ngenes=1 #for testing function without loop - comment this out

      mydata<-makedata(nsnp,ngenes,effsize,nsnpeff,LDbase)
       print(paste('nsnp',nsnp,'ngenes',ngenes,'effsize',effsize,'nsnpeff',nsnpeff))
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
       write.csv(mydata,dataname,row.names=FALSE)
      ggcorrplot(cor(mydata),type='lower') #check we have correct correlation structure
      
         }
      }
     }
  }
  rm(mydata) #free up space in memory by removing file after it is saved
                    
```
Having created the data, we can now look at power and false positive rates for different  parameter sets and analyses. We are interested in the trade-off between these: which is the most efficient method for identifying true effects while avoiding false positives?

Our particular interest is in GSCA. This uses maximum likelihood estimation and takes a long time to run (days rather than hours). The script is provided below, but in general we just rely on reading in the saved data that can then be compared with results from other methods.

```{r doGSCAchunk}
dogsca <- 0
if (dogsca == 1){ #set to zero to skip this chunk
  mycondition<-0
  cluster<-0 #previous versions had option of cluster 1 for clustered SNPs in gene
  #----------------------------------------------------------------------
  # For each of nrun runs take a sample and analyse
  #----------------------------------------------------------------------
 
    for (nsnpg in nsnp_per_gene){
    for (effsize in effsizelist){
      for(nsnpeff in nsnpefflist){ #cycle by N SNPs with effect
        for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
print(paste('nsnp',nsnp,'ngenes',ngenes,'effsize',effsize,'nsnpeff',nsnpeff))
   
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
      myread<-data.frame(read_csv(dataname))
     nruns <-1 #have to limit N runs so that simulation runs in reasonable time
 
      for (myrun in 1:nruns){ #take new sample on each run
       for (nsub in nsublist){
  
       #do GSCA - but just read in prior results unless you can wait many days!
    
      myrows<-sample(npop,nsub) 
      mysample<-myread[myrows,-(nsnp+1)] #skip latent phenotype
      
      #use ASGSCA to analyse
      snprange<-1:nsnp
      phenrange<-((nsnp+1):(nsnp+3))
      ObservedVar=colnames(mysample)[c(snprange,phenrange)] #skip latent phenotype - not used in analysis
      
      LatentVar=c(paste0("gene",1:ngenes),"Neurodev")
      
      #W0 is I x L matrix where rows are genotypes and traits, columns are genes and latent phenotype
      W0=matrix(rep(0,length(LatentVar)*(n2sim+nsnp)),nrow=n2sim+nsnp,ncol=length(LatentVar), dimnames=list(ObservedVar,LatentVar))
      
      if(ngenes==1)
        {W0[1:nsnp,1] = 1}
       gene_break<-floor(nsnp/ngenes)

      ind<-1:ngenes*gene_break
      #placer=ifelse(length(ind)<2,NA,c(1:ind[1],seq((ind[i-1]+1):ind[2],(ind[2]+1):ind[3],(ind[3]+1):ind[4]))
      
      for(w in 1:ngenes)
      {
        if(w==1)
        {W0[1:ind[1],w] = 1}
        if(w>1){W0[(ind[w-1]+1):ind[w],w] = 1}
      }
      W0[(nsnp+1):(nsnp+3),(ngenes+1)]=1
      
      
      #B0 is L x L matrix with 0s and 1s, where 1 indicates arrow from latent geno in row to latent phenotype in column
      B0=matrix(rep(0,(ngenes+1)*(ngenes+1)),nrow=(ngenes+1),ncol=(ngenes+1), dimnames=list(LatentVar,LatentVar))
      B0[1:ngenes,(ngenes+1)]=1
      
      # GSCA(mysample,W0, B0,latent.names=LatentVar, estim=TRUE,path.test=FALSE,path=NULL,nperm=1000)
      # for quick scrutiny of weights use this -but for pvalues need slow version using path.test
      
      myfit<-GSCA(mysample,W0, B0,latent.names=LatentVar, estim=TRUE,path.test=TRUE,path=NULL,nperm=mynperm)
      
      runsummary<-c(myfit$Path[1:ngenes,ngenes+1],myfit$pvalues[1:ngenes,ngenes+1])
      runsummary<-data.frame(runsummary)
      endrow<-startrow+ngenes-1
      
      bigsummary[startrow,1:8]<-c(myrun,cluster,nsnp,ngenes,nsnpeff,effsize,nsub,mycondition)
      bigsummary$gene[startrow:endrow]<-1:ngenes
      bigsummary$path[startrow:endrow]<-runsummary[1:ngenes,]
      bigsummary$p[startrow:endrow]<-runsummary[(ngenes+1):(2*ngenes),]
      startrow<-endrow+1 #update startrow for next run
      }
    }
   }
  }
 }

}
t2<-toc()
print(t2)
bigsummary$sig.p<-0
w<-which(bigsummary$p<.05)
bigsummary$sig.p[w]<-1
bigsummary<-bigsummary[1:endrow,]
#fill in blank rows
for (r in 1:endrow){
  if(is.na(bigsummary$run[r]))
  {bigsummary[r,1:9]<-bigsummary[(r-1),1:9]}
}

saveRDS(bigsummary,bigsumname)
#Flag up significant p=values - depending on whether 1 or 2 genes need adjusted pvalues
bigsummary$sig.p.cor<-0
w<-which(bigsummary$p<.025 & bigsummary$ngene==2)
bigsummary$sig.p.cor[w]<-1
w2<-which(bigsummary$p<.05 & bigsummary$ngene==1)
bigsummary$sig.p.cor[w2]<-1
}

#if skipping this chunk, read in prior data summary

if (dogsca==0){
   bigtable<-read_csv('PT_simulation_results_MC_adj_formatted-1.csv')
}
 #add columns to hold comparison data from lm analysis
bigtable$lm0<-NA
bigtable$lm1<-NA
bigtable$lm2<-NA
bigtable$lm3<-NA
bigtable$lm.any<-NA

bigtable2<-filter(bigtable,ngenes==1,cluster==0)
bigtable2<-dplyr::select(bigtable2,-c(nsig2,nsig3,nsig4,cluster))
bigtable2$key<-paste0(bigtable2$nsnp,bigtable2$nseff,bigtable2$ngenes,bigtable2$eff_size,bigtable2$Npats)
lmcolstart<-which(colnames(bigtable2)=='lm0')
```


The simplest method of analysis is to use linear model to estimate the regression coefficient for the function predicting phenotype from N major alleles. Conventionally, this would be done with a Bonferroni correction to adjust for multiple testing, by dividing the p-value (typically .05) by the N SNPs tested. This is equivalent to the method used in PLINK. Note, however, that this is less stringent than a method that also corrects for the N phenotypes. But if the phenotypes are correlated, Bonferroni correction would be too stringent.

For each combination of parameters that we have simulated, we compute for each gene, the N runs giving a significant association in any gene, when true effect size is zero, or when it is greater than zero.

```{r lm.function}
dolm<-function(myread,myruns,myN){
  myposresults<-matrix(0,nrow=myruns,ncol=3) #to holds counts of sig p value for each phenotype measure
  for (r in 1:myruns){
   
          myrows<-sample(npop,myN) 
          mysample<-myread[myrows,] 

          #For each phenotype measure, we are just going to search for the largest correlation with a SNP and test that
           mybonfp<-.05/nsnp
          for (j in (nsnp+2):(nsnp+4)){ #remember to skip latent phenotype
            myposresults[r,(j-nsnp-1)]<-0 #initialise matrix to hold sigs
            allcor<-cor(mysample)
            maxr<-max(abs(allcor[1:nsnp,j]))
            w<-which(abs(allcor[,j])==maxr) #finds the SNP with highest correl
            myfit<-lm(mysample[,j]~mysample[,w]) 
            myp<-summary(myfit)[[4]][8] #gives assoc pvalue
            if (myp<mybonfp){
              myposresults[r,(j-nsnp-1)]<-1
            }
            
            }
          
  }
  
return(myposresults)}
```

```{r assoctests}

 tic() #start timer
 for (nsnpg in nsnp_per_gene){
    for (effsize in effsizelist){
      for(nsnpeff in nsnpefflist){ #cycle by *proportion* of SNPs with effect rather than N SNPs
        for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
       
#    nsnp=20;effsize=0.1;nsnpeff=5;ngenes=1 #for testing function without loop - comment this out
    
   
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
      myread<-data.frame(read_csv(dataname))
      myruns<-100
     for (myN in c(100,500)){ #N subjects in each run
     
     myposresults<-data.frame(dolm(myread,myruns,myN))
     myposresults[,4]<-myposresults[,1]+myposresults[,2]+myposresults[,3]
     #find row in bigtable that matches these parameters
     matchkey<-paste0(nsnp,nsnpeff,ngenes,effsize,myN)
     w<-which(bigtable2$key==matchkey)
     lmsump<-table(myposresults[,4])
     #v clunky but got fed up trying to do this elegantly
     #codes the N runs where 0, 1,2 or 3 phenotypes give significant assoc
     #(with bonferroni correction applied)
     bigtable2[w,(lmcolstart)]<-length(which(myposresults[,4]==0))
     bigtable2[w,(lmcolstart+1)]<-length(which(myposresults[,4]==1))
     bigtable2[w,(lmcolstart+2)]<-length(which(myposresults[,4]==2))
     bigtable2[w,(lmcolstart+3)]<-length(which(myposresults[,4]==3))
           }
        }
      }
    }
 }
bigtable2$lm.any<-bigtable2$nrun-bigtable2$lm0
```

We now have a big table (bigtable2) that has results from GSCA and linear regression (PLINK-like) results aligned, so we can compare sensitivity/specificity of the two methods with equivalent data.

We will do a plot for each effect size/NSNP/samplesize of FP rate (%null runs with significant result) on y-axis, and power (%true assoc with sig results) on x-axis - contrasting the two methods.

One additional question is whether we can predict results by just computing an overall effect size - would mean effect size across all SNPs work? If so, should we average r, or sqrt(r2)?
If the answer is YES, then we can greatly simplify the simulations by using mean ES, which would be a function of nsnp, nseff and eff_size. (At least for the one-gene situation)

We'll look at this first, just using the LM results. Let's compute mean ES.
Twitter advice is to go for r (or at least the slope: since standardized beta coefficient =  correlation of the predictor with the outcome, we can average r.

```{r meaneffsize}
#since r is equivalent to standardized beta, we will average these
bigtable2$meanES <-(bigtable2$nseff*bigtable2$eff_size)/bigtable2$nsnp
bigtable2$Ncolor<-1
w<-which(bigtable2$Npats==500)
bigtable2$Ncolor[w]<-2
bigtable2$snpshape<-14+bigtable2$nsnp/20
w<-which(bigtable2$nsnp==100)
bigtable2$snpshape[w]<-17
par(mfrow=c(1,2))
plot(bigtable2$lm0~bigtable2$meanES,ylim=c(0,100),col=bigtable2$Ncolor,pch=bigtable2$snpshape)
#now do comparison with GSCA
bigtable2$gscanull<-100-bigtable2$nsig1
plot(bigtable2$gscanull~bigtable2$meanES,ylim=c(0,100),type='n')
text(bigtable2$gscanull~bigtable2$meanES,labels=bigtable2$nsnp,col=bigtable2$Ncolor,cex=.8)
plot(bigtable2$gscanull~bigtable2$lm0,col=bigtable2$Ncolor,pch=bigtable2$snpshape)
```













