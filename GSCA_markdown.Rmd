---
title: "GSCApaper_markdown"
author: "DVM Bishop"
date: "31/05/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(doBy)
library(tidyr)
library(tidyverse)
library(MASS)
library(stats)
library(doSNOW)
library(foreach)
library(ASGSCA)
library(matrixcalc)
library(tictoc)
library(ggcorrplot)
library(ggplot2)
library(psych) #useful for dfOrder

options(scipen = 999) #turn off scientific notation
```

## R Markdown

Combines various scripts used for the paper. 
Based on *simulating gsca data via latent variable_v2.R* from OSF.

Data simulated either by (a) generating data with given correlation structure; or (b) Romdhani method. (PT has done version b, but it is not yet in this paper)
Varied:
N SNPs; 
N SNPs with an effect on phenotype; 
N participants; 
N genes; (i.e. N gene pathways in GSCA)
effect size, i.e. size of correlation between SNPs with an effect and phenotypes. 
Here used small effect size (r=.1) and large (r=.4)
Also contrasted situations where the SNPs with an effect were clustered within a gene, or spread across genes.

Because GSCA is very slow, did 100 runs for each combination (this takes several weeks to run through all combinations)



```{r initialise.vars}

nrun<-100
startrow<-1 #row of big summary to write to
mynperm=100 #for GSCA - probably need more than this but to test prog use 100 for speed; tends to give values pretty close to those obtained with 1000 permutations
dogsca<-1
npop<-10000 #size of population to simulate; on each run we sample from this population
n2sim<-3 #n phenos indexing latent pheno factor 

LDbase <- .75 #This is an index that controls the correlation between adjacent SNPs - correl will decline with distance
#original file had LDbase of .9. Later I tried with reduced value of .75 - seems to give plausible values.
#Heatmap will allow you to visualise these

phencorr<-.75 #intercorrelation of 3 phenotypes 
#with this setting, intercorrelation between phenotypes is around .56

#List of effectsizes to cycle through (multiplied by 10)
effsizelist<-c(1,2,4) #will be divided by 10 to give a correlation
#(just given here as whole number to make easier to use in filename)

#List of values to cycle through, specifies N SNPs with effect on phenotype
nsnpefflist<-c(0,5,10) #N of SNPs with an effect on phenotype (NB in some versions of script we specify a proportion - but in this version we use N. Note that for a given nsnp, proportion will decrease as N SNPs increases, e.g. 10/100 SNPs is only 10% with an effect, where 10/20 SNPs is 50%).

#NB we use value of zero for testing false positive rate (alternative would be to set effect size to zero)

ngenelist<-c(1)  #for comparison with lm we just look at case with single gene
#Script should work with larger N genes, but this has not yet been tested with current rmd file (though it was OK in earlier R script) 

nsublist<-c(100,500) #number of cases in the simulation - simulation will cycle through all values listed here

nsnp_per_gene<-c(20,40) #N SNPs that are tested - only a subset of these will have an effect on the phenotype - defined by nsnpefflist. If just one gene, then these correspond to all SNPs in the simulation.

#make name for this big summary file - aim to specify all variable parameters in the name.
#will be saved in a directory called 'simulated data'
bignamebase<-paste0('bigsum_nsnp',list(nsnp_per_gene),'eff',list(effsizelist),'neff',list(nsnpefflist), 'ngene',list(ngenelist),'_nsub',list(nsublist),'nrun',nrun,'_nperm',mynperm,'_phencor',100*phencorr,'_LD',LDbase)

bignamebase<-str_replace_all(bignamebase,"\\)","_") %>%  #substitute _ for close bracket
str_replace_all("c\\(","") %>% #remove c(
str_replace_all("\\,","|") %>% #replace comma with |
str_replace_all(' ','') #remove blanks

bigname<-paste0('simulated_data/',bignamebase,'.csv')
  
effsizelist<-effsizelist/10 #turn effect size (specified above) into a correlation

```

```{r createdataframeforsummary}
bigsumNrow<-nrun*length(effsizelist)*length(nsnpefflist)*length(nsublist)*sum(ngenelist)*length(nsnp_per_gene)
#NB we have to do 'sum' for ngenelist, as there is a row for each gene
bigsummary<-data.frame(matrix(NA,ncol=13,nrow=bigsumNrow))
colnames(bigsummary)<-c('run','cluster','nsnp','ngenes','nsnpeff','effsize','nsub','condition','gene','path','p','sig.p','sig.p.cor')

```

We start by simulating a large population with the desired correlational structure. 

We have 3 language phenotypes, which are intercorrelated at level phencorr.
(This differs from previous version which had a latent phenotype)

We create a correlation matrix for the desired N SNPs. Initially this is just a matrix of zeros with ones on diagonal, but to have more realistic data, we can simulate a degree of linkage disequilibrium, i.e. adjacent SNPs are correlated.
The variable LDbase is used to control this - a value of .75 gives realistic-looking values. 

Default assumption is no correlation of SNPs with latent factor; then specified number of SNPs have effect added by changing their correlation with latent value to equal the specific effect size.

Initially, the SNP values are just simulated together using mvrnorm to give a matrix of z-scores. This matrix will have the SNPs correlated according to the setting of LDbase, and individual SNPs will also be correlated with the phenotypes, with level of effect size, as specified above.

We then need to adjust the SNPs scores so that they correspond to values of 0, 1 or 2 - i.e. N major alleles. We select a random value between .25 and .5 for minor allele frequency for each SNP, and from this we can compute the proportion of each genotype in the population.  We allocate a score of 0, 1 or 2, depending on the z-score for the simulated data (using normal distribution of probabilities). E.g. if we expected 50% of cases to have AA genotype (equivalent to SNP score of 2) we would allocate a score of 2 when the z-score was greater than zero (p = .5). 

The end result of this step is a dataframe consisting of a row for each simulated person in the population (currently set at 10,000 cases), which contains values of 0, 1 or 2 for each SNP, and values for 3 observed phenotypes. This should have the desired correlation structure - which can be observed in a heatmap using  ggcorrplot(cor(mydata)).

The simulated data is saved so it can be used to compare different analyses.

          

```{r makedatafunction}   
#This function will be called in the next block. Need to run this first.
makedata <- function(nsnp,ngenes,effsize,nsnpeff,LDbase){
    gpcov<-effsize #effect size of SNPs with effect, set as correlation 
    snpnames<-paste0('snp', 1:nsnp)
    maf<-runif(nsnp,.25,.5) #minor allele freq for each SNP; set to random value from .25 to .5
    
    #Setup a dataframe to hold to hold simulated data.
    mydata<-data.frame(matrix(nrow=npop,ncol=(3+sum(nsnp)))) #set to hold npop cases - we will sample from these for each run
    #We have +3 to allow for additional columns for  3 phenotype variables
    #SNP values will be 0, 1 or 2, but we start with random normal numbers
    
    #-----------------------------------------------------------------
    #simulate the SNPS as correlated zscores, correlation is mycorr
    #-----------------------------------------------------------------
    #Where there is correlation, assume correlation depends on how close SNPs are, 
    #achieved by making size of correlation proportional to distance
    # (where sequence order is proxy for distance)
    
    
    h <-nsnp+3 #additional columns used for phenotypes 
    phenrange<-(nsnp+1):(nsnp+3) #range of cols for phenotypes
    mymean<-rep(0,h) #vector of means to use in mvrnorm function
    mycorr<-0 #default is uncorrelated
    mycov2<-matrix(mycorr,nrow=h,ncol=h) #cov matrix for mvrnorm
     mycov2[(nsnp+1):(nsnp+3),(nsnp+1):(nsnp+3)]<-phencorr #intercorrelation between phenotypes
   
    diag(mycov2)<-rep(1,h) #ones on diagonal of cov matrix
    gene_break<-floor(nsnp/ngenes) #new gene starts after every gene-break value

    #Now specify correlation between SNPs, i.e. linkage disequilibrium       
    if(LDbase>0){ #need to compute correlation for conditions where SNP effects clustered in genes. Can ignore if no linkage disequilibrium
      for (i in 1:h){
        irange<-1+as.integer((i-1)/gene_break) #irange specifies haplotype block for i
        for (j in 1:h){
          jrange<- 1+as.integer((j-1)/gene_break) #jrange specifies haplotype block for j
          if(irange==jrange){
            k<- abs(i-j)
            thisr<-ifelse(nsnp<=20,LDbase-gene_break*k/100,(LDbase-gene_break*k/((nsnp^2)/(10*ngenes)))) #tweaked so magnitude of correlation declines with distance between SNPs (PT: further tweak to allow different nsnp)
            if(thisr<0){thisr<-0}
            mycov2[i,j]<-thisr
            mycov2[j,i]<-mycov2[i,j] #correlation determined by distance between SNPs!
            if(k==0){mycov2[i,j]<-1}
          }
        }
      }
    }
    
    #Identify which SNPs are correlated with latent phenotype
    #Here assume effects divided equally betewen genes
    
    nsnp_g<-round(nsnp/ngenes,0)
    nsnpeff_g<-round(nsnpeff/ngenes,0)
    snpeff_pos<-vector() #initialise vector for holding indices of SNPs with effect
    for (g in 1:ngenes){
      myrange<-(1+(g-1)*nsnp_g):(g*nsnp_g) #range of indices for SNPs in this gene
      snpeff_pos<-c(snpeff_pos,sample(myrange,nsnpeff_g,replace=F)) #randomly selected SNPs in this range
    }
    
    mycov2[snpeff_pos,phenrange]<-effsize #for SNPs with effect, add effect size in cov matrix
    mycov2[phenrange,snpeff_pos]<-effsize
    if(matrixcalc::is.positive.definite(mycov2)==FALSE) #ensure matrix is positive definite
    {mycov2<-Matrix::nearPD(mycov2,keepDiag=TRUE)$mat}
    
    #create simulated data for SNPs and latent phenotype only
    mymean<-rep(0,h)
    mydata=data.frame(mvrnorm(n = npop, mymean, mycov2))
      #################################################
    
    colnames(mydata)[1:nsnp]<-snpnames
    colnames(mydata)[phenrange]<-c('nonword','elang','rlang')
  
    #-------------------------------------------------------------------------
    #Convert gene scores to integers: 0-2 for N SNPs with major allele
    
    firstcol<-1
    lastcol<-nsnp
    p<-c(0,0,0) #initialise a vector to hold p values for different N alleles
    for (i in 1:nsnp){
      p[1]<-(1-maf[i])^2 #proportion homozygous for major allele
      p[2]<-2*(1-maf[i])*maf[i] #proportion heterozygous
      p[3]<-maf[i]^2 #proportion homozygous for minor allele
      
      #now use p-values to assign z-score cutoffs that convert to 0,1,2 minor alleles
      temp<-mydata[,i] #column of z-scores for this SNP
      w<-which(temp<qnorm(p[1]))
      mydata[w,i]<-0
      w<-which(temp>qnorm(p[1]))
      mydata[w,i]<-1
      w<-which(temp>qnorm(p[2]+p[1]))
      mydata[w,i]<-2
    }
    return(mydata) 
  }
```           
 


```{r adddata}

#ensure makedata function is loaded before running this chunk
 mycondition<-0 #initialise
 tic() #start timer
  for (nsnpg in nsnp_per_gene){
     for (effsize in effsizelist){
       for(nsnpeff in nsnpefflist){ #cycle by N SNPs with an effect
         for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
       

      mydata<-makedata(nsnp,ngenes,effsize,nsnpeff,LDbase)
       print(paste('nsnp',nsnp,'ngenes',ngenes,'effsize',effsize,'nsnpeff',nsnpeff))
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
       write.csv(mydata,dataname,row.names=FALSE)
      ggcorrplot(cor(mydata),type='lower') #check we have correct correlation structure
      
         }
      }
     }
  }
  rm(mydata) #free up space in memory by removing file after it is saved
                    
```
Having created the data, we can now look at power and false positive rates for different  parameter sets and analyses. We are interested in the trade-off between these: which is the most efficient method for identifying true effects while avoiding false positives?

Our particular interest is in GSCA. This uses maximum likelihood estimation and takes a long time to run (days rather than hours). The script is provided below, but in general we just rely on reading in the saved data that can then be compared with results from other methods.

```{r doGSCAchunk}

if (dogsca == 1){ #set to zero to skip this chunk
  mycondition<-0
  cluster<-0 #previous versions had option of cluster 1 for clustered SNPs in gene
  #----------------------------------------------------------------------
  # For each of nrun runs take a sample and analyse
  #----------------------------------------------------------------------
 
    for (nsnpg in nsnp_per_gene){
    for (effsize in effsizelist){
      for(nsnpeff in nsnpefflist){ #cycle by N SNPs with effect
        for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
print(paste('nsnp',nsnp,'ngenes',ngenes,'effsize',effsize,'nsnpeff',nsnpeff))
   
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
      mydata<-data.frame(read_csv(dataname))
  #   nruns <-1 #have to limit N runs so that simulation runs in reasonable time
 
      for (myrun in 1:nrun){ #take new sample on each run
       for (nsub in nsublist){
  
       #do GSCA - but just read in prior results unless you can wait many days!
    
      myrows<-sample(npop,nsub) 
       mysample<-mydata[myrows,]
      #use ASGSCA to analyse
      snprange<-1:nsnp
      phenrange<-((nsnp+1):(nsnp+3))
      ObservedVar=colnames(mysample)[c(snprange,phenrange)] #skip latent phenotype - not used in analysis
      
      LatentVar=c(paste0("gene",1:ngenes),"Neurodev")
      
      #W0 is I x L matrix where rows are genotypes and traits, columns are genes and latent phenotype
      W0=matrix(rep(0,length(LatentVar)*(n2sim+nsnp)),nrow=n2sim+nsnp,ncol=length(LatentVar), dimnames=list(ObservedVar,LatentVar))
      
      if(ngenes==1)
        {W0[1:nsnp,1] = 1}
       gene_break<-floor(nsnp/ngenes)

      ind<-1:ngenes*gene_break
      #placer=ifelse(length(ind)<2,NA,c(1:ind[1],seq((ind[i-1]+1):ind[2],(ind[2]+1):ind[3],(ind[3]+1):ind[4]))
      
      for(w in 1:ngenes)
      {
        if(w==1)
        {W0[1:ind[1],w] = 1}
        if(w>1){W0[(ind[w-1]+1):ind[w],w] = 1}
      }
      W0[(nsnp+1):(nsnp+3),(ngenes+1)]=1
      
      
      #B0 is L x L matrix with 0s and 1s, where 1 indicates arrow from latent geno in row to latent phenotype in column
      B0=matrix(rep(0,(ngenes+1)*(ngenes+1)),nrow=(ngenes+1),ncol=(ngenes+1), dimnames=list(LatentVar,LatentVar))
      B0[1:ngenes,(ngenes+1)]=1
      
      # GSCA(mysample,W0, B0,latent.names=LatentVar, estim=TRUE,path.test=FALSE,path=NULL,nperm=1000)
      # for quick scrutiny of weights use this -but for pvalues need slow version using path.test
      
      myfit<-GSCA(mysample,W0, B0,latent.names=LatentVar, estim=TRUE,path.test=TRUE,path=NULL,nperm=mynperm)
      
      runsummary<-c(myfit$Path[1:ngenes,ngenes+1],myfit$pvalues[1:ngenes,ngenes+1])
      runsummary<-data.frame(runsummary)
      endrow<-startrow+ngenes-1
      
      bigsummary[startrow,1:8]<-c(myrun,cluster,nsnp,ngenes,nsnpeff,effsize,nsub,mycondition)
      bigsummary$gene[startrow:endrow]<-1:ngenes
      bigsummary$path[startrow:endrow]<-runsummary[1:ngenes,]
      bigsummary$p[startrow:endrow]<-runsummary[(ngenes+1):(2*ngenes),]
      startrow<-endrow+1 #update startrow for next run
      }
    }
   }
  }
 }

}
t2<-toc()
print(t2)

write_csv(bigsummary,bigname)
}
if (dogsca==0){
  bigsummary<-read_csv(bigname)
}

bigsummary$sig.p<-0
w<-which(bigsummary$p<.05)
bigsummary$sig.p[w]<-1
#bigsummary<-bigsummary[1:endrow,] #? needed when multiple genes to remove blanks
#fill in blank rows
# for (r in 1:endrow){
#   if(is.na(bigsummary$run[r]))
#   {bigsummary[r,1:9]<-bigsummary[(r-1),1:9]}
# }

#saveRDS(bigsummary,bigname)
#Flag up significant p=values - depending on whether 1 or 2 genes need adjusted pvalues
bigsummary$sig.p.cor<-0
w<-which(bigsummary$p<.0125 & bigsummary$ngenes==4)
if(length(w)>0){
bigsummary$sig.p.cor[w]<-1
}
w<-which(bigsummary$p<.025 & bigsummary$ngenes==2)
if(length(w)>0){
bigsummary$sig.p.cor[w]<-1
}
w2<-which(bigsummary$p<.05 & bigsummary$ngenes==1)
bigsummary$sig.p.cor[w2]<-1
bigsummary$LD<-LDbase

write_csv(bigsummary,bigname)

```

We've created bigsummary, which contains results from each run, which in turn is based on nperm simulations using GSCA.
The parameters for each run are identified in columns.
We are now going to average across runs of each condition type, to estimate %significant runs.


```{r groupsummary}
#Filenames specified here. We can just bolt these together, but we need to separate them according to LDbase value.


bigsummary<-read_csv(bigname)

by_N_neff_nsub <- bigsummary %>% group_by(nsub,cluster, ngenes,gene,nsnp,nsnpeff,effsize,condition)
#this means that when summarising, will group by these columns

groupedsummary_new<-by_N_neff_nsub %>% summarise(power = mean(sig.p.cor))

#groupedsummary_new$cond2<-c(1,2,1,2,3,4,3,4,5,6,5,6,7,8,7,8,9,10,9,10,
#                        11,12,11,12,13,14,13,14,15,16,15,16,17,18,17,18,
#                        19,20,19,20,21,22,21,22,23,24,23,24)

groupedsummary_new$nsnpeffplot<-groupedsummary_new$nsnpeff+groupedsummary_new$ngenes/100-.02

groupedsummary_new$nsnpfac<-as.factor(groupedsummary_new$nsnp)
groupedsummary_new$ngenes<-as.factor(groupedsummary_new$ngenes)
groupedsummary_new$nsub<-as.factor(groupedsummary_new$nsub)
groupedsummary_new$gene<-as.factor(groupedsummary_new$gene)



ggplot(groupedsummary_new,aes(x=nsnpeffplot,y=power,colour=ngenes,shape=nsub))+geom_point()+geom_line(aes(linetype=gene))+facet_grid(nsnpfac~effsize)+theme_bw()+theme(legend.position = "bottom")

bigname2<-paste0('grp_',bignamebase,'.csv')

write_csv(groupedsummary_new,bigname2)

#

```

```{r checkgrouped}
#Pick a particular cell and check values from bigsummary.
#Try effsize = .1
temp<-filter(bigsummary,effsize==.1,nsub==100,nsnp==20,nsnpeff==5)
aggregate(temp$sig.p.cor, by=list(temp$nsub,temp$nsnp,temp$nsnpeff),FUN=mean)

```



```{r readgrouped}

 bigtable<-groupedsummary_new
 #initialise additional columns to hold comparison data from lm analysis
#We'll be saving the N analyses where there are 0, 1, 2, or 3 significant phenotypes from any SNP, and also the N analyses where there is any significant phenotype (i.e. all cases except zero)
bigtable$lm0<-NA
bigtable$lm1<-NA
bigtable$lm2<-NA
bigtable$lm3<-NA
bigtable$lm.any<-NA
bigtable$nrun<-nrun

#remove unwanted rows and columns
bigtable2<-filter(bigtable,ngenes==1,cluster==0)

bigtable2$key<-paste0(bigtable2$nsnp,bigtable2$nsnpeff,bigtable2$ngenes,bigtable2$effsize,bigtable2$nsub) #unique key from combination of conditions - easy way to align rows from same conditions for different methods
lmcolstart<-which(colnames(bigtable2)=='lm0')

```


The simplest method of analysis is to use linear model to estimate the regression coefficient for the function predicting phenotype from N major alleles. Conventionally, this would be done with a Bonferroni correction to adjust for multiple testing, by dividing the p-value (typically .05) by the N SNPs tested. This is equivalent to the method usually used in PLINK. Note, however, that this is less stringent than a method that also corrects for the N phenotypes. But if the phenotypes are correlated (as in this case), Bonferroni correction would be too stringent.

For each combination of parameters that we have simulated, we compute for each gene, the N runs giving a significant association for any SNP, when true effect size is zero, or when it is greater than zero.

```{r lm.function}
#This function is run in the next chunk. 
dolm<-function(myread,myruns,myN){
  myposresults<-matrix(0,nrow=myruns,ncol=3) #to holds counts of sig p value for each phenotype measure
  for (r in 1:myruns){
   
    myrows<-sample(npop,myN) #from the big population file, select myN rows at random
    mysample<-myread[myrows,] #subsample for analysis with myN rows
    #For each phenotype measure, we are just going to search for the largest correlation with a SNP and test that for significance
           mybonfp<-.05/nsnp #Bonferroni corrected p-value
          for (j in (nsnp+1):(nsnp+3)){ # phenotypes
            myposresults[r,(j-nsnp)]<-0 #initialise matrix to hold sigs
            allcor<-cor(mysample) #correlation matrix for all SNPs and phenotypes
            maxr<-max(abs(allcor[1:nsnp,j])) 
            w<-which(abs(allcor[,j])==maxr) #finds the SNP with highest correl for phenotype j
            myfit<-lm(mysample[,j]~mysample[,w]) 
            myp<-summary(myfit)[[4]][8] #gives associated pvalue (4 and 8 just happen to be the indices where p-value recorded in summary)
            if (myp<mybonfp){
              myposresults[r,(j-nsnp)]<-1 #myposresults will be matrix
              #with row for each run, and 3 columns that will show 1 if there was a
              #significant association with a SNP and that phenotype on that run.
            }
     }
  }
  
return(myposresults)}
```

```{r assoctests}

 tic() #start timer
 for (nsnpg in nsnp_per_gene){
    for (effsize in effsizelist){
      for(nsnpeff in nsnpefflist){ #cycle by N SNPs with effect 
        for (ngenes in ngenelist){
          nsnp<-nsnpg*ngenes #compatibility with previous version where nsnp was all in analysis summed over genes
       
#    nsnp=20;effsize=0.1;nsnpeff=5;ngenes=1 #for testing function without loop - comment this out
    
   #read previously simulated data for this combination of parameters
       dataname<-paste0('simulated_data//rawsim_nsnp',nsnp,'_ngene',ngenes,'_effsize',effsize,'_nsnpeff',nsnpeff,'_phencorr',phencorr,'_LDbase',LDbase,'.csv')
      myread<-data.frame(read_csv(dataname))
      myruns<-100
     for (myN in c(100,500)){ #N subjects in each run
     
     myposresults<-data.frame(dolm(myread,myruns,myN))
     myposresults[,4]<-myposresults[,1]+myposresults[,2]+myposresults[,3]
      #4th column counts the N phenotypes where a significant SNP occurred
     
     #find row in bigtable that matches these parameters
     matchkey<-paste0(nsnp,nsnpeff,ngenes,effsize,myN)
     w<-which(bigtable2$key==matchkey)
     lmsump<-table(myposresults[,4]) 
     #v clunky but got fed up trying to do this elegantly
     #codes the N runs where 0, 1,2 or 3 phenotypes give significant assoc
     #(with bonferroni correction applied)
     bigtable2[w,(lmcolstart)]<-length(which(myposresults[,4]==0))
     bigtable2[w,(lmcolstart+1)]<-length(which(myposresults[,4]==1))
     bigtable2[w,(lmcolstart+2)]<-length(which(myposresults[,4]==2))
     bigtable2[w,(lmcolstart+3)]<-length(which(myposresults[,4]==3))
           }
        }
      }
    }
 }
bigtable2$lm.any<-bigtable2$nrun-bigtable2$lm0 #runs where any of the 3 phenotypes was significantly associated with any of the SNPs
```

We now have a big table (bigtable2) that has results from GSCA and linear regression (PLINK-like) results aligned, so we can compare sensitivity/specificity of the two methods with equivalent data.

We will do a plot for each effect size/NSNP/samplesize of %significant result on y-axis, and true effect size on x-axis - contrasting the two methods.

This raises the question of whether we can predict results by just computing an overall effect size - would mean effect size across all SNPs work? 
If the answer is YES, then we can greatly simplify the simulations by using mean ES, which would be a function of nsnp, nseff and eff_size. (At least for the one-gene situation)

We'll look at this first, just using the LM results. Let's compute mean ES.
Twitter advice is to go for r (this reflects the regression slope, since standardized beta coefficient =  correlation of the predictor with the outcome). So we can average r across all SNPs in the analysis.

```{r meaneffsize}
#This chunk needs correction - seems it thinks nsnp is a factor and this stops it computing ES.
#since r is equivalent to standardized beta, we will average these
bigtable2$meanES <-(bigtable2$nsnpeff*bigtable2$effsize)/bigtable2$nsnp
#with baseR it is easiest just to create columns to code colour and shape of symbols in plot . There are better ways to do this!
bigtable2$Ncolor<-1
w<-which(bigtable2$Npats==500)
bigtable2$Ncolor[w]<-3
w<-which(bigtable2$nseff==10)
bigtable2$Ncolor[w]<-bigtable2$Ncolor+1
bigtable2$snpshape<-14+bigtable2$nsnp/20
w<-which(bigtable2$nsnp==100)
bigtable2$snpshape[w]<-17

#Set up plot to show side by side
par(mfrow=c(1,2))
plot((100-bigtable2$lm0)~bigtable2$meanES,ylim=c(0,100),col=bigtable2$Ncolor,pch=bigtable2$snpshape,xlab='Mean Effect size',ylab='% significant runs',main='Linear Regression with Bonferroni')
legend(.1, 55, legend=c("N=100", "N=500"),
       col=c(1,2), lty=c(1,1),cex=0.8)
legend(.1, 35, legend=c("nsnp=20", "nsnp=40","nsnp=100"),
      pch=c(15,16,17), cex=0.8)
abline(v=0) #Where true effect size = 0, value > 0 indicates false positive
#now do comparison with GSCA

plot((bigtable2$power*100)~bigtable2$meanES,ylim=c(0,100),col=bigtable2$Ncolor,pch=bigtable2$snpshape,xlab='Mean Effect size',ylab='% significant runs',main='GSCA')
legend(.1, 55, legend=c("N=100", "N=500"),
       col=c(1,2), lty=c(1,1), cex=0.8)
legend(.1, 35, legend=c("nsnp=20", "nsnp=40","nsnp=100"),
      pch=c(15,16,17), cex=0.8)
text(0.1,65,'Where true effect size = 0,\n yaxis > 0 indicates false positive;\n otherwise yaxis indicates power',cex=.7)
abline(v=0) #Where true effect size = 0, value > 0 indicates false positive
#text(bigtable2$gscanull~bigtable2$meanES,labels=bigtable2$nsnp,col=bigtable2$Ncolor,cex=.8) 
#plot(bigtable2$gscanull~bigtable2$lm0,col=bigtable2$Ncolor,pch=bigtable2$snpshape)

plot(bigtable2$lm.any,bigtable2$power*100)
#alternative method of computing mean ES from r^2
#newdata$altmeanES<-sqrt((newdata$nseff*newdata$eff_size^2)/newdata$nsnp)

```


```{r check}
temp<-filter(bigsummary,ngenes==1)
plot(bigsummary$p,bigsummary$path)
plot(temp$p,temp$path)
temp<-filter(temp,nsnpeff==5)
plot(temp$p,temp$path)
temp<-filter(temp,effsize==.1)
plot(temp$p,temp$path)



aggregate(temp$sig.p.cor, by=list(temp$nsnpeff,temp$nsub,temp$nsnp),FUN=mean)



```



